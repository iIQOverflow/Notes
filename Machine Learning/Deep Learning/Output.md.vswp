vnote_backup_file_826537664 C:/Users/90388/OneDrive/git_repository/Notes/Machine Learning/Deep Learning/Dropout.md
# Dropout

## 神经网络中的Dropout
神经网络训练中随机忽视一些神经元，这些被忽视的神经元前向以及后向传播都不参与。

更确切地讲，在训练阶段，每个节点都以$1-p$地可能性被去除，以$p$的可能性保留，最后剩下“精简“的神经网络，被去除的节点的输入和输出也被去除了。

## 为什么需要Dropout
缓解过拟合。
## 理解Dropout
机器学习中，正则化是缓解过拟合的一种方法，正则化通过向损失函数中增加惩罚项减少过拟合现象，模型因此在训练过程中减少学习相互依赖的特征权重。

Dropout是一种在神经网络中正则化的方法。

### 训练阶段：
对于每个**隐藏层**，每个**训练样本**，每次**迭代**，忽略（归零）节点的随机分数p（和相应的激活）
### 训练阶段：
使用所有的激活，但将它们减少一个因子p（以解释训练期间缺少的激活）

![dropout](_v_images/20190103172804094_6075.png =450x)
## 结论
1. Dropout强制神经网络学习更强大的功能，这些功能与其他神经元的许多不同随机子集结合使用。
2. Dropout大约使收敛所需的迭代次数加倍，但是与之相对的，每次迭代的时间也减少了。
3. Hge隐藏层，每个隐藏层都可能被去除，我们就有$2^H$个可能的模型，在测试阶段，考虑整个网络，每次激活减少因子p。
## 参考
[Dropout in (Deep) Machine learning](https://medium.com/@amarbudhiraja/https-medium-com-amarbudhiraja-learning-less-to-learn-better-dropout-in-deep-machine-learning-74334da4bfc5)
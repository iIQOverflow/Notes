# Image classification

数据集：CIFAR-10

## 搭建最简单版本的CNN（baseline）
输入：4维tensor，尺寸为（128，32，32，3），分别表示一批图片的个数128，图片的宽的像素点个数32，高的像素点个数32和信道个数3（每个像素点有3个数）。

卷积神经网络的计算过程：
1. 卷积层1
2. 卷积层2
3. 卷积层3
4. 全连接层
5. 分类层：隐藏单元10，激活函数softmax。

参数初始化，所有权重矩阵使用random_normal(0.0, 0.001)，所有偏置向量使用constant(0.0)。使用**corss entropy**作为目标函数，使用**Adam梯度下降法**进行参数更新，学习率设为固定值0.001。

## 数据增强
训练数据增加微小的扰动或变化，一方面可以增加训练数据，从而提高模型的泛化能力，另一方面可以增加噪声数据，从而增强模型的鲁棒性。主要的数据增强方法有：翻转变换 flip、随机修剪（random crop）、色彩抖动（color jittering）、平移变换（shift）、尺度变换（scale）、对比度变换（contrast）、噪声扰动（noise）、旋转变换/反射变换 （rotation/reflection）等，可以参考Keras的官方文档 [2] 。获取一个batch的训练数据，进行数据增强步骤之后再送入网络进行训练。

主要做的操作有：
1. **图像切割**：生成比图像尺寸小一些的矩形框，对图像进行随机的切割，最终以矩形框内的图像作为训练数据。
2. **图像翻转**：对图像进行左右翻转。
3. **图像白化**：对图像进行白化操作，即将图像本身归一化成Gaussian(0,1)分布。

这里图像白化>图像切割>图像翻转。
## 从模型入手
1. 权重衰减（weight decay）：对于目标函数加入正则化项，限制权重参数的个数，这是一种防止过拟合的方法，这个方法其实就是机器学习中的l2正则化方法，只不过在神经网络中旧瓶装新酒改名为weight decay。
2. 在每次训练的时候，让某些的特征检测器停过工作，即让神经元以一定的概率不被激活，这样可以防止过拟合，提高泛化能力。
3. 批正则化（batch normalization）：batch normalization对神经网络的每一层的输入数据都进行正则化处理，这样有利于让数据的分布更加均匀，不会出现所有数据都会导致神经元的激活，或者所有数据都不会导致神经元的激活，这是一种数据标准化方法，能够提升模型的拟合能力。
4. LRN：LRN层模仿生物神经系统的侧抑制机制，对局部神经元的活动创建竞争机制，使得响应比较大的值相对更大，提高模型泛化能力。

## 变化学习率

1. 首先使用较大的学习率进行训练，观察目标函数值和验证集准确率的收敛曲线。
2. 如果目标函数值下降速度和验证集准确率上升速度出现减缓时，减小学习率。
3. 循环步骤2，直到减小学习率也不会影响目标函数下降或验证集准确率上升为止。

## 加深网络层数
网络层数过大，梯度进一步衰减，导致网络性能下降。

## 残差网络
由于网络层数加深，误差反向传播的过程中会使梯度不断地衰减，而通过跨层的直连边，可以使误差在反传的过程中减少衰减。


## 参考
[如何一步一步提高图像分类准确率？](https://zhuanlan.zhihu.com/p/29534841)

[cifar10-tensorflow](https://github.com/persistforever/cifar10-tensorflow)
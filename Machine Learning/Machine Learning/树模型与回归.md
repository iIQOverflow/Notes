# 树模型与线性回归

## 树模型与线性回归比较
* 线性回归模型的解释性是决策树、随机森林、xgboost无法比拟的，也无法取代。

* 线性回归可以建立线性模型，而xgboost是不可以的。举个例子，即使是简单的y=x+1的线性关系，xgboost也无法做到。

* 线性模型计算简单，适用于快速部署。

## 决策树与逻辑回归比较
### 分歧
* 逻辑回归对数据**整体结构**的分析优于决策树，而决策树对**局部结构**的分析优于逻辑回归。
* 逻辑回归擅长辨识线性关系，而决策树对线性关系的把握较差。线性联系在实践中有很多优点：简介，易理解，可以在一定程度上防止对数据的过度拟合。决策树的强项是非线性关联，但是很多非线性关系完全可以用直线模拟，而且效果很好。
* 逻辑回归对极值比较敏感，容易受极端值的影响，而决策树对极值有很好的抗干扰性。
### 算法逻辑
决策树由于采用分割的方法，所以能够深入数据细部，但同时就失去了对全局的把握。一个分层一旦形成，它和别的层面或节点的关系就被切断了，以后的挖掘只能在局部中进行。同时由于切分，样本数量不断萎缩，所以无法支持对多变量的同时检验。而逻辑回归，始终着眼整个数据的拟合，所以对全局把握较好。但无法兼顾局部数据，或者说缺乏探查局部结构的内在机制。
### 应用区别
决策树的结果和逻辑回归相比略显粗糙。逻辑回归原则上可以提供数据中每个观察点的概率，而决策树只能把挖掘对象分为有限的组群。比如决策树确定17个节点，全部人口就只能有17个概率，在应用上受到一定限制。就操作来说，决策树比较容易上手，需要的数据预处理较少，而逻辑回归则要求一定的训练和技巧。
### 互补
#### 思路
利用决策树对局部数据结构优越的把握能力增加逻辑回归的效力。
#### 嫁接
* 从决策树分析中找出数据局部结构，作为在逻辑回归中构建依变量（interaction)的依据。
* 在需要对预测因子进行离散化处理时，利用决策树分析决定最佳切分点。
* **决策树分类的最终结果作为预测变量，和其他协变量一起代入回归模型，又称为“嫁接式模型”**。从理论上讲，嫁接模型综合了决策树和逻辑回归的优点。最终节点包含了数据中重要的局部结构，而协变量可以拾补被决策树遗漏的数据整体结构。
#### 嫁接局限
嫁接模型是一个很巧妙的设计，但是在实践中并没有得到普遍的认同。由于决策树已经对数据进行了最大限度的拟合，所以留给协变量的余地很小。换句话说，把决策树的最终节点作为预测因子，就可能找不出还有独立作用的协变量。而没有协变量，逻辑回归实际只是决策树的重复。再有，由于节点是多个属性的综合，不易解释。每个节点到底代表什么不明确，由此限制了这种方法的推广。


## 参考
[既然xgboost那么好，线性回归还有存在的价值吗？](http://sofasofa.io/forum_main_post.php?postid=1001268)
[比较决策树和回归 ](http://blog.sina.com.cn/s/blog_652090850100gwxl.html)